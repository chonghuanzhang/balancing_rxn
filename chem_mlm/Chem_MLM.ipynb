{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQjjs7FjkCiA"
      },
      "source": [
        "# Config environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXxKFlCbhI9F",
        "outputId": "5e85664c-cc50-433a-a98d-1959f748f4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/masked_smiles\n",
            "Fri Feb  3 09:55:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    25W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/reaction_balancer'\n",
        "import sys\n",
        "sys.path.append(path)\n",
        "%cd /content/drive/MyDrive/reaction_balancer\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdV7AOyKh9PL",
        "outputId": "be4015e9-f3bd-4731-8d03-5f05df13538d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.8 MB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 59.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182 kB 80.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.5-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.9 MB 34.0 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182 kB 74.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168 kB 76.4 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168 kB 82.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166 kB 72.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166 kB 80.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162 kB 82.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162 kB 51.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158 kB 84.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 86.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 87.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 83.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 71.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 85.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 81.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 84.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 156 kB 85.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=1e91e9d24fce0e1cef17c3f92de2a1d6d55d5475c0b142d7cfc0da9f9492cb43\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.29 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 wandb-0.13.5\n"
          ]
        }
      ],
      "source": [
        "# %%bash\n",
        "!pip install torch torchvision\n",
        "!pip install pandas\n",
        "!pip install transformers\n",
        "!pip install wandb\n",
        "\n",
        "from src.main_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAJBI-ijqYnc"
      },
      "source": [
        "# Config wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Veflna8i4Xqz"
      },
      "source": [
        "wandb config with HuggingFace\n",
        "\n",
        "https://docs.wandb.ai/guides/integrations/huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "ElmQFeI2phdk",
        "outputId": "fe92563b-4a33-44d5-a9a0-02b6ccbd3b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchonghuanzhang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/masked_smiles/wandb/run-20221204_163631-sawe9sem</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/chonghuanzhang/masked_smiles/runs/sawe9sem\" target=\"_blank\">lemon-silence-91</a></strong> to <a href=\"https://wandb.ai/chonghuanzhang/masked_smiles\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/chonghuanzhang/masked_smiles/runs/sawe9sem?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f2ba0088f10>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!wandb login\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnbn-It14ffd"
      },
      "source": [
        "# Define dataset and method to learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPFLcQDnRSut"
      },
      "source": [
        "Datasets:\n",
        "\n",
        "(1) learn from uspto: uspto_sentences.txt, uspto_masked_smiles.csv\n",
        "\n",
        "(2) learn from reaxys + uspto: complete_sentences.txt, complete_masked_smiles.csv\n",
        "\n",
        "(3) learn from uspto_cleaned: uspto_cleaned_sentence.txt, uspto_cleaned_masked_smiles.csv\n",
        "\n",
        "This is the cleaned data by removing unmapped reactant into reagents, and passed through the ChemBalancer\n",
        "\n",
        "\n",
        "\n",
        "Methods:\n",
        "\n",
        "(1) random mask: conventional way to mask 15% of reaction SMILES dynamically, no matter if that crosses \".\" and \">>\".\n",
        "\n",
        "(2) default mask: mask the specific defined molecule in reaction SMILES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JqCr1hN4eH2"
      },
      "outputs": [],
      "source": [
        "learn_from = 'uspto' # 'complete'\n",
        "\n",
        "mask_method = 'random' #'default'\n",
        "\n",
        "# trial number\n",
        "trial_num = 6\n",
        "\n",
        "# Choose if the ChemMLM model is LHS ChemMLM or LHS ChemMLM.\n",
        "mask_side = 'right' # 'left'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LBMTZ7CQpuq"
      },
      "outputs": [],
      "source": [
        "data_file =  'data/{}_sentences.txt'.format(learn_from)\n",
        "tokenizer_file = \"tokenizer/{}_custom_tokenizer\".format(learn_from)\n",
        "pretrain_tokenizer_file = \"tokenizer/{}_tokenizer\".format(learn_from)\n",
        "if mask_side == 'right':\n",
        "    masked_data_file = 'data/{}_masked_token_smiles.csv'.format(learn_from)\n",
        "else:\n",
        "    masked_data_file = 'data/{}_masked_token_smiles_lhs.csv'.format(learn_from)\n",
        "\n",
        "# From saved tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrain_tokenizer_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B86N8zj7KFm"
      },
      "source": [
        "# Tokenize reaction SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgzBtdNeBNEi"
      },
      "outputs": [],
      "source": [
        "tokenizer = mlm_tokenizer(data_file, tokenizer_file, pretrain_tokenizer_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikehuzaKjKl1"
      },
      "source": [
        "# Train masked lanuage model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4EG8RqMkFPp",
        "outputId": "bbf9734d-c729-4091-9894-b74918be83b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.25.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config = RobertaConfig(\n",
        "    vocab_size=52000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "configuration = RobertaConfig()\n",
        "print(configuration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tK15FBkkrpQ"
      },
      "source": [
        "ÂÖ∑‰ΩìÂèØ‰ª•ÈÖçÁΩÆÂì™‰∫õÂèÇÊï∞ÂèØ‰ª•ÈÄöËøáÈªòËÆ§ÂèÇÊï∞ÁöÑÂÆö‰πâÊù•Ëá™Ë°åÂÆö‰πâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vClRO19olApz",
        "outputId": "e5ecc2d4-915a-4d75-e9e8-1f8ab543f52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83504416\n"
          ]
        }
      ],
      "source": [
        "model = RobertaForMaskedLM(config=config)\n",
        "\n",
        "print(model.num_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWwZBwNVLpCA"
      },
      "source": [
        "TrainingArgumentsÈáåËÆæÁΩÆgradient_accumulation_steps = 10 Âàôbatch size ÂèØ‰ª•ËÆæÁΩÆ‰∏∫160*10 = 160 Êú¨Ë¥®‰∏äÂ∞±ÊòØ16‰∏™Ê†∑Êú¨ÁÆó‰∏ÄÊ¨°Ê¢ØÂ∫¶Ôºå‰ΩÜÊòØÁ¥ØËÆ°ËÆ°ÁÆó‰∫Ü10Ê¨°‰πãÂêéÊâçÂèçÂêë‰º†Êí≠ ËøôÊ†∑ÂæóÂà∞ÁöÑÊïàÊûúÂíå‰∏ÄÊ¨°ÊÄßÁÆó160‰∏™Ê†∑Êú¨ÁöÑÊ¢ØÂ∫¶Âπ∂ÂèçÂêë‰º†Êí≠ÊòØ‰∏ÄÊ†∑ÁöÑ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWipIe71japA",
        "outputId": "f4323705-d3a4-4fd4-a5f4-13395618a7e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model/{}_{}_mlm_#{}\".format(learn_from, mask_method, trial_num),\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=200,\n",
        "    per_device_train_batch_size=16,\n",
        "    save_steps=20000,\n",
        "    save_total_limit=None,\n",
        "    remove_unused_columns=False,\n",
        "    learning_rate=2e-04,\n",
        "    gradient_accumulation_steps=1,\n",
        "    # evaludation\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=5000,\n",
        "    eval_accumulation_steps=1,\n",
        "    # wandb\n",
        "    report_to=\"wandb\",  # enable logging to W&B\n",
        "    run_name=\"{}_{}_mlm_#{}\".format(learn_from, mask_method, trial_num)  # name of the W&B run (optional)\n",
        ")\n",
        "\n",
        "if mask_method == 'default':\n",
        "    data_collator = CustomDataCollatorForLanguageModeling(tokenizer)\n",
        "\n",
        "    dataset_trn, dataset_evl, dataset_tst = get_dataset(\n",
        "    mask_method, masked_data_file, data_file, tokenizer)\n",
        "\n",
        "elif mask_method == 'random':\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=True, mlm_probability=0.5)\n",
        "\n",
        "    if learn_from == 'complete':\n",
        "        dataset_trn, dataset_evl = get_complete_data(mask_method, masked_data_file, data_file, tokenizer, mode='train')\n",
        "        # dataset_trn = pd.read_pickle(os.path.join('./data/complete_sentences/', 'dataset_trn.pickle'))\n",
        "        # dataset_evl = pd.read_pickle(os.path.join('./data/complete_sentences/', 'dataset_evl.pickle'))\n",
        "        # dataset_tst = pd.read_pickle(os.path.join('./data/complete_sentences/', 'dataset_tst.pickle'))\n",
        "\n",
        "    else:\n",
        "        # get data\n",
        "        dataset_trn, dataset_evl, dataset_tst = get_dataset(\n",
        "            mask_method, masked_data_file, data_file, tokenizer)\n",
        "\n",
        "#DefaultDataCollator(return_tensors='pt')\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    # dataset=dataset,\n",
        "    train_dataset=dataset_trn,\n",
        "    eval_dataset=dataset_evl,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXka9HpKjf6o",
        "outputId": "fe93996a-a67c-4ebf-9778-c5d9bc853cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[2863,   58,   34,  ..., 2861, 2861, 2861],\n",
            "        [ 273, 2863, 2863,  ..., 2861, 2861, 2861],\n",
            "        [2863, 2863,   46,  ..., 2861, 2861, 2861],\n",
            "        ...,\n",
            "        [ 257, 2863, 2863,  ..., 2861, 2861, 2861],\n",
            "        [  45, 2594, 2863,  ..., 2861, 2861, 2861],\n",
            "        [ 257,    7,   34,  ..., 2863, 2302, 2863]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  34, -100,   34,  ..., -100, -100, -100],\n",
            "        [-100,   16,  256,  ..., -100, -100, -100],\n",
            "        [ 287,  259, -100,  ..., -100, -100, -100],\n",
            "        ...,\n",
            "        [-100,  259,   46,  ..., -100, -100, -100],\n",
            "        [  45,    2,  265,  ..., -100, -100, -100],\n",
            "        [-100, -100, -100,  ...,   46,   28,   46]])}\n",
            "input_ids\n",
            "torch.Size([16, 231])\n",
            "attention_mask\n",
            "torch.Size([16, 231])\n",
            "labels\n",
            "torch.Size([16, 231])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "dd = trainer.get_train_dataloader()\n",
        "ddd = next(iter(dd))\n",
        "print(ddd)\n",
        "\n",
        "for k in ddd:\n",
        "    print(k)\n",
        "    print(ddd[k].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "1be82758f8e04d14977b2b72b54879d8"
          ]
        },
        "id": "92H9ygixjp2O",
        "outputId": "5149ed0a-c426-4f28-dad3-ccea5a66b4fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from model/uspto_random_mlm_#6/checkpoint-3600000.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 449755\n",
            "  Num Epochs = 200\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5622000\n",
            "  Number of trainable parameters = 83504416\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 128\n",
            "  Continuing training from global step 3600000\n",
            "  Will skip the first 128 epochs then the first 1920 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1be82758f8e04d14977b2b72b54879d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1920 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3655215' max='5622000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3655215/5622000 7:30:03 < 267:11:55, 2.04 it/s, Epoch 130.03/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3605000</td>\n",
              "      <td>0.053100</td>\n",
              "      <td>0.051214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3610000</td>\n",
              "      <td>0.052400</td>\n",
              "      <td>0.050244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3615000</td>\n",
              "      <td>0.052600</td>\n",
              "      <td>0.050595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3620000</td>\n",
              "      <td>0.051200</td>\n",
              "      <td>0.050926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3625000</td>\n",
              "      <td>0.052800</td>\n",
              "      <td>0.050425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3630000</td>\n",
              "      <td>0.051400</td>\n",
              "      <td>0.051423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3635000</td>\n",
              "      <td>0.051700</td>\n",
              "      <td>0.049991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3640000</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>0.049604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3645000</td>\n",
              "      <td>0.055700</td>\n",
              "      <td>0.050464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650000</td>\n",
              "      <td>0.054200</td>\n",
              "      <td>0.050287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3655000</td>\n",
              "      <td>0.052500</td>\n",
              "      <td>0.051044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to model/uspto_random_mlm_#6/checkpoint-3620000\n",
            "Configuration saved in model/uspto_random_mlm_#6/checkpoint-3620000/config.json\n",
            "Model weights saved in model/uspto_random_mlm_#6/checkpoint-3620000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to model/uspto_random_mlm_#6/checkpoint-3640000\n",
            "Configuration saved in model/uspto_random_mlm_#6/checkpoint-3640000/config.json\n",
            "Model weights saved in model/uspto_random_mlm_#6/checkpoint-3640000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24986\n",
            "  Batch size = 8\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# trainer.train()\n",
        "trainer.train(\"model/uspto_random_mlm_#6/checkpoint-3600000\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvWMDFqwlpFS"
      },
      "outputs": [],
      "source": [
        "# trainer.evaluate()\n",
        "trainer.save_model(\"model/{}_{}_mlm_#{}\".format(learn_from, mask_method, trial_num))\n",
        "wandb.finish()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk46zkcqrYcj"
      },
      "source": [
        "# From saved model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeLJTwsYoC_O"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"model/{}_{}_mlm_#{}\".format(learn_from, mask_method, trial_num),\n",
        "    # model=\"model/complete_random_mlm_#0/checkpoint-7660000\",\n",
        "    # tokenizer=\"tokenizer/{}_tokenizer\".format('complete'),num_workers=12,device=0,batch_size=64\n",
        "    tokenizer=\"tokenizer/{}_tokenizer\".format(learn_from),num_workers=8,device=0,batch_size=64\n",
        ")\n",
        "\n",
        "if mask_method == 'random':\n",
        "    # get data\n",
        "    test_size = 0.1\n",
        "    if learn_from == 'complete':\n",
        "        test_size = 0.01\n",
        "    dataset_trn, dataset_evl, dataset_tst = get_dataset(\n",
        "        'default', masked_data_file, data_file, tokenizer, test_size=test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Lamg7Sj79G"
      },
      "source": [
        "# Training set results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMSIoYZvrW8m",
        "outputId": "105b95bb-04fa-4ba6-d565-6c18b3b25c9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 100/574232 [00:02<3:34:05, 44.69it/s]\n"
          ]
        }
      ],
      "source": [
        "inputs_trn = [i['text'] for i in dataset_trn][1000::]\n",
        "label_trn = [i['label'] for i in dataset_trn][1000::]\n",
        "\n",
        "results= []\n",
        "from tqdm import tqdm\n",
        "i = 0\n",
        "for item in tqdm(inputs_trn):\n",
        "    results.append(fill_mask(item))\n",
        "    i+=1\n",
        "    if i>100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2V6RDO6tfjR",
        "outputId": "65af688a-992f-4f09-c504-17a8a98d442e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  CO\n",
            "ground truth = CO\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  I\n",
            "ground truth = I\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  F\n",
            "ground truth = F\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Br\n",
            "ground truth = Br\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  F\n",
            "ground truth = F\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "the correct rate for one-token molecule is 1.0\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(1,100):\n",
        "    if type(results[i][0])==dict:\n",
        "        print('prediction = ',results[i][0]['token_str'])\n",
        "        print('ground truth =',label_trn[i])\n",
        "        print('\\n')\n",
        "        total+=1\n",
        "        if results[i][0]['token_str']==label_trn[i]:\n",
        "            correct+=1\n",
        "print('the correct rate for one-token molecule is {}'.format(correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_OF9Xo0uFli",
        "outputId": "1d98dd7e-adc3-474d-865b-8f5dd3fd6fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prediction =  N#N\n",
            "ground truth = N#N\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  CCCC(=O)O\n",
            "ground truth = CCCC(=O)O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  CCccC\n",
            "ground truth = CCCC(=O)O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  I.I\n",
            "ground truth = [I-]\n",
            "\n",
            "\n",
            "prediction =  O-2+2\n",
            "ground truth = [O-2]\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  C=CC(=O)O\n",
            "ground truth = C=CC(=O)O\n",
            "\n",
            "\n",
            "the correct rate for middle length token molecule is 0.8125\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(1,100):\n",
        "    if type(results[i][0])!=dict:\n",
        "\n",
        "        predict = [item for item in results[i]]\n",
        "\n",
        "        if len(predict)<=10:\n",
        "          predict = ''.join([item[0]['token_str'] for item in predict])\n",
        "          print('prediction = ',predict)\n",
        "          print('ground truth =',label_trn[i])\n",
        "          print('\\n')\n",
        "          total+=1\n",
        "          if predict==label_trn[i]:\n",
        "              correct+=1\n",
        "print('the correct rate for middle length token molecule is {}'.format(correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCVX_exb4Gct",
        "outputId": "f7bb7efa-7e8c-4f7a-99f7-845e1e260b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prediction =  FC(F)(F)c()(cccc(12)2)2)c1\n",
            "ground truth = OC(CNc1cccc(Oc2ccccc2)c1)C(F)(F)F\n",
            "\n",
            "\n",
            "prediction =  FC(F)(F)c1cccc((((((((FFFFFF(F(FFFFFF))2)c1\n",
            "ground truth = FC(F)(F)c1cccc(CN2CCCN(c3ccc4nnc(C(F)(F)F)n4n3)CC2)c1\n",
            "\n",
            "\n",
            "prediction =  CC(C)c1cc(-c2ccccccccccc2)c2ncccc2c1\n",
            "ground truth = CC(C)c1cc(-c2ccc(F)c(C=O)c2)c2ncccc2c1\n",
            "\n",
            "\n",
            "prediction =  COC(O)c1ccc(O((((N(C))C)cc1\n",
            "ground truth = COC(=O)c1ccc(NS(=O)(=O)N(C)C)cc1\n",
            "\n",
            "\n",
            "prediction =  COc1ccc2c3c1O[[[[[C[CCHCHCCCCCCCHCCCCCCCCCCCCCCCCCCC\n",
            "ground truth = COc1ccc2c3c1O[C@H]1[C@@H](O)C=C[C@H]4[C@@H](C2)N(C)CC[C@]314\n",
            "\n",
            "\n",
            "prediction =  O=c1[nH]ccc)c222ccccccccccc22222222(F)F)cccc1F\n",
            "ground truth = O=c1[nH]cc(Cl)c2c1c(-c1ccc(N3CCOCC3)cc1)nn2-c1c(F)cccc1F\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)c1nc(N)cn1CC1CC1\n",
            "ground truth = CCOC(=O)c1nc(N)cn1CC1CC1\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)CCN(Cc1ccccc1)Cc1ccccc1\n",
            "ground truth = CCOC(=O)CCN(Cc1ccccc1)Cc1ccccc1\n",
            "\n",
            "\n",
            "prediction =  COc1ccc2c3c(c(c(c()))))CC))))))((()))O)c1-3\n",
            "ground truth = COc1ccc2c3c(c(NCCN(C)C)nc2c1)C(=O)c1ccnc(NCCN(C)C)c1-3\n",
            "\n",
            "\n",
            "prediction =  O=CN(O)CCc1cncc)))))cccccccccccc(((((Cl)cc2)CC1\n",
            "ground truth = O=CN(O)C(CCc1cncc(Cl)c1)CS(=O)(=O)N1CCN(c2ccc(Cl)cc2)CC1\n",
            "\n",
            "\n",
            "prediction =  O=c1c2c(c3333333333ccccc3=3)C2\n",
            "ground truth = O=c1c2c(c3cccnc3n1-c1ccccc1)OC(CO)C2\n",
            "\n",
            "\n",
            "prediction =  N#Cc1cccc2CCC2O2CCOOOOOOOOOO)))22)2ccccc1)3)ccccc21\n",
            "ground truth = N#Cc1cccc2c1CCC2NC(=O)CC(NS(=O)(=O)c1ccc2ccccc2c1)c1ccccc1\n",
            "\n",
            "\n",
            "prediction =  O=S(=O)(c1c)11112c22CCCCC2)cc1\n",
            "ground truth = O=S(=O)(c1ccc(Cl)cc1)N1CCCC1c1ccsc1\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)[C@@](C)(C)CCCCCNNNNNNNNNNcc2222ccccc22c2ccccc21\n",
            "ground truth = CCOC(=O)[C@@](C)(F)C(=O)N[C@@H]1C(=O)N(C)c2ccccc2-c2ccccc21\n",
            "\n",
            "\n",
            "prediction =  CCCc1nc(CC)c2c(=c(=cccccccccccccNCCNCCNCCNCCNCCN)CC3)ccc3OCC)nn12\n",
            "ground truth = CCCc1nc(CC)c2c(=O)[nH]c(-c3cc(S(=O)(=O)N4CCN(C)CC4)ccc3OCC)nn12\n",
            "\n",
            "\n",
            "prediction =  CC(C)OC(=O)C111111111111111111111c1111cccc(=(=(=(=)))c2)cc1\n",
            "ground truth = CC(C)OC(=O)C1CCCN1S(=O)(=O)c1ccc(N2CCC(NCC(O)COc3cccc4[nH]c(=O)[nH]c34)CC2)cc1\n",
            "\n",
            "\n",
            "prediction =  CCCCC(CC)COc1ccc(C(c)c2ccccc2)cc1\n",
            "ground truth = CCCCC(CC)COc1ccc(CC(CN)c2ccccc2)cc1\n",
            "\n",
            "\n",
            "prediction =  NS(=O)(=O)c1cc1cc(Cl)ccc2s1\n",
            "ground truth = NS(=O)(=O)c1cc2cc(Cl)ccc2s1\n",
            "\n",
            "\n",
            "prediction =  CC(C)C(NC=O)OOOOccccOc(((((2)C(=O)O\n",
            "ground truth = CC(C)C(Cc1cccc(Oc2ccc(F)cc2)n1)(NC=O)C(=O)O\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)c1ccc(NC(=(=)NCC2CC2)cc1\n",
            "ground truth = CCOC(=O)c1ccc(NC(=O)NCC2CC2)cc1\n",
            "\n",
            "\n",
            "prediction =  CC1CCC(N2ccc(c2ccOOOOOOO333N4NNNN443333333CC2)c2C)c1\n",
            "ground truth = Cc1cnc(N2CCN(C(=O)c3ccc(N4C(=O)CCC4C)cc3N3CCCS3(=O)=O)CC2)c(C)c1\n",
            "\n",
            "\n",
            "prediction =  CNc1c1c(=O)O(((((((((FFFFFFFO)O)nO\n",
            "ground truth = CNc1c(NCCCCCc2csc(NC(N)=NCC(F)(F)F)n2)c(=O)c1=O\n",
            "\n",
            "\n",
            "prediction =  N#CCC(=O)c1ccc(F)cc1\n",
            "ground truth = N#CCC(=O)c1ccc(F)cc1\n",
            "\n",
            "\n",
            "prediction =  COC(=O)c1c(N)NN+](=O)[O-])c1\n",
            "ground truth = COC(=S)c1cc(C)ccc1[N+](=O)[O-]\n",
            "\n",
            "\n",
            "prediction =  COc1nc(C)ccc1-c11ccc2CCCCCCCCC3)n2)[nH]n1\n",
            "ground truth = COc1nc(C)ccc1-c1cc(Nc2cncc(O[C@@H]3CCCNC3)n2)[nH]n1\n",
            "\n",
            "\n",
            "prediction =  C=Cc1cccc(c2ncnc(CCCCC)c2)c1\n",
            "ground truth = C#Cc1cccc(Nc2ncnc3c2ccn3CC=C)c1\n",
            "\n",
            "\n",
            "prediction =  CCCc1c(O)c1c2ccccccccccc)))))))))))))))))))2)c2ncnn12\n",
            "ground truth = CCCc1c(Cc2ccc(-c3ccccc3C#N)cc2)c(=O)n(C2CCC3(CC2)OCC(C)(C)O3)c2ncnn12\n",
            "\n",
            "\n",
            "prediction =  COc1ccccc1CC(NC)O(=OOOOOOcccccccccccccc)C2O)O\n",
            "ground truth = COc1ccccc1CC(Nc1nc2ccc(Cl)cc2cc1C(=O)O)C(=O)O\n",
            "\n",
            "\n",
            "prediction =  CC(Nc1nc(Cl(F))22211111c1)(F)cc1\n",
            "ground truth = CC(Nc1nc(Nc2cccc(O)c2)ncc1F)c1ccc(F)cc1\n",
            "\n",
            "\n",
            "prediction =  C(=O)c1ccc(Cc2Cc2222222OOOCCCCCCCCCCCCCCCCCOOOOH)C2O)cc1\n",
            "ground truth = C#Cc1ccc(Cc2ccccc2O[C@@H]2O[C@H](COC(=O)OC)[C@@H](O)[C@H](O)[C@H]2O)cc1\n",
            "\n",
            "\n",
            "prediction =  CCC(=O)NC1cc2ccccccccccccccccccccccccccccccccccc))))cnHcc)cc1F\n",
            "ground truth = CCC(=O)N[C@H]1CC[C@H](NC(=O)c2c(C)[nH]c3c(-c4cc(F)c(OC)cc4OCC4CC4)ccnc23)CC1\n",
            "\n",
            "\n",
            "prediction =  O=C(O)c1c(cc)cccccc)C(=O)C1=21\n",
            "ground truth = O=C(O)CN1C(=O)c2ccc(I)cc2C1=O\n",
            "\n",
            "\n",
            "prediction =  O=Cc1cc(F)ccc1Br\n",
            "ground truth = O=Cc1cc(F)ccc1Br\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)c1ccc(C#CcCcCccccccccccccccccCCCC)C)cO)cc1\n",
            "ground truth = CCOC(=O)c1ccc(C#Cc2ccc3c(c2)C(C)(C)CCC3=O)cc1\n",
            "\n",
            "\n",
            "prediction =  NC(=O)c1cc(-c22222ccccccCCNCCNCCNCCNCCNCCNOOO))CCCC)CCCCCCCC)33)))c[nH]c12\n",
            "ground truth = NC(=O)c1cc(-c2ccc(C=O)s2)cc2c(C3CCN(S(=O)(=O)CCCN4CCCC4)CC3)c[nH]c12\n",
            "\n",
            "\n",
            "prediction =  CC(C)C[C@H](H1111111111(=(=(=(=(=(=(=)111c1cccc2ccccc1\n",
            "ground truth = CC(C)C[C@@H](CN1CCC[C@H]1C(=O)NCCCCc1ccccc1)NC(=O)c1cccc2ccccc12\n",
            "\n",
            "\n",
            "prediction =  CN1CCN(CCCS(=O)O)CCCN)CC1\n",
            "ground truth = CN1CCN(CCCS(=O)(=O)O)CC1\n",
            "\n",
            "\n",
            "prediction =  O=C1c2c2[222223)3)nc2c12\n",
            "ground truth = O=c1[nH]c2ccc(O)cc2nc1-c1cccs1\n",
            "\n",
            "\n",
            "prediction =  O=C(COc1c1O1OOOOOOOOOOOOOONNNNcccccccO)(=O)c1ccccc1\n",
            "ground truth = O=C(COc1ccc(S(=O)(=O)Cl)cc1OCC(=O)OCCS(=O)(=O)c1ccccc1)OCCS(=O)(=O)c1ccccc1\n",
            "\n",
            "\n",
            "prediction =  O=S1(=O2(2222(()(CCO)CCN2CCO\n",
            "ground truth = O=S1(=O)CC2C(C1)N(CCO)CCN2CCO\n",
            "\n",
            "\n",
            "prediction =  Nc#Cc(-c2)2)ccccc1\n",
            "ground truth = Nc1cc(-c2ccccc2)ns1\n",
            "\n",
            "\n",
            "prediction =  CC1(C)S[CC2HHH22222CCCCCCCNONOOOOO11)))Cl(ClCl)ccccc1\n",
            "ground truth = CC1(CNc2ccccc2)S[C@@H]2C(NC(=O)Cc3ccccc3)C(=O)N2C1C(=O)OCC(Cl)(Cl)Cl\n",
            "\n",
            "\n",
            "prediction =  CC(=O)CC(=O)Nc11111NNNNNNCCC(C)O)CC1\n",
            "ground truth = CC(=O)CC(=O)Nc1cnc2c(c1)CC(N(C)CCc1ccccc1)CC2\n",
            "\n",
            "\n",
            "prediction =  CCC(=O)N(CCCCCN)C(CCccCClCl)c(Cl)c1\n",
            "ground truth = CCC(=O)N(CCCCCN(C)C)c1ccc(Cl)c(Cl)c1\n",
            "\n",
            "\n",
            "prediction =  CC(C)(C)[C1C1CC)C)))c)11111)1111111111111111212222Oc12.O=O\n",
            "ground truth = CC(C)(C)[Si](C)(C)O[C@@H](CN(CCc1ccc(N)cc1)Cc1ccccc1)c1ccc(OCc2ccccc2)c2[nH]c(=O)ccc12\n",
            "\n",
            "\n",
            "prediction =  COc1cccc(-c(=O)C2CCCCCCCccccccccc))))))))))N32)c1\n",
            "ground truth = COc1cccc(-c2cccc3c2C(=Cc2[nH]c(C)cc2C(=O)NCCn2ccnn2)C(=O)N3)c1\n",
            "\n",
            "\n",
            "prediction =  CCCCc(=O)Nc1cc(Cl2Cl)ClClCl)c)c)OOOOOOOOOOOOOOOOOOCCCCC(CCC)C)cc1F\n",
            "ground truth = CCCCc1nn(-c2cc(NC(C)=O)ccc2Cl)c(=O)n1Cc1ccc(-c2cc(CCC)ccc2S(=O)(=O)NC(C)(C)C)cc1F\n",
            "\n",
            "\n",
            "the correct rate for long length token molecule is 0.0851063829787234\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(1,100):\n",
        "    if type(results[i][0])!=dict:\n",
        "\n",
        "        predict = [item for item in results[i]]\n",
        "\n",
        "        if len(predict)>10:\n",
        "          predict = ''.join([item[0]['token_str'] for item in predict])\n",
        "          print('prediction = ',predict)\n",
        "          print('ground truth =',label_trn[i])\n",
        "          print('\\n')\n",
        "          total+=1\n",
        "          if predict==label_trn[i]:\n",
        "              correct+=1\n",
        "print('the correct rate for long length token molecule is {}'.format(correct/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5c7S3P8ntTQ"
      },
      "source": [
        "# Test set results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_fH80SO5ef3",
        "outputId": "d62fdf14-28c3-4ed4-aa44-c70f019a8059"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 100/79880 [00:03<41:07, 32.33it/s]\n"
          ]
        }
      ],
      "source": [
        "inputs_tst = [i['text'] for i in dataset_tst][300::]\n",
        "label_tst = [i['label'] for i in dataset_tst][300::]\n",
        "\n",
        "results= []\n",
        "from tqdm import tqdm\n",
        "i = 0\n",
        "for item in tqdm(inputs_tst):\n",
        "    results.append(fill_mask(item))\n",
        "    i+=1\n",
        "    if i>100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6vaDZGI8nDb",
        "outputId": "4be0ebfc-2bdb-486a-dd03-7f88abbf56d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prediction =  2\n",
            "ground truth = CCO\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Br\n",
            "ground truth = Br\n",
            "\n",
            "\n",
            "prediction =  F\n",
            "ground truth = F\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  CCO\n",
            "ground truth = CCO\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  CCO\n",
            "ground truth = CCO\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Br\n",
            "ground truth = Br\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  I\n",
            "ground truth = I\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  Br\n",
            "ground truth = Br\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "prediction =  O\n",
            "ground truth = O\n",
            "\n",
            "\n",
            "prediction =  Cl\n",
            "ground truth = Cl\n",
            "\n",
            "\n",
            "the correct rate for one-token molecule is 0.9259259259259259\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(1,100):\n",
        "    if type(results[i][0])==dict:\n",
        "        print('prediction = ',results[i][0]['token_str'])\n",
        "        print('ground truth =',label_tst[i])\n",
        "        print('\\n')\n",
        "        total+=1\n",
        "        if results[i][0]['token_str']==label_tst[i]:\n",
        "            correct+=1\n",
        "print('the correct rate for one-token molecule is {}'.format(correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq8PnLHGPI86",
        "outputId": "45012bde-f8b3-4bc2-f0f4-edb8a7e2dc3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prediction =  CC(=O)O\n",
            "ground truth = [H][H]\n",
            "\n",
            "\n",
            "prediction =  O.[Mn].O\n",
            "ground truth = [SbH3]\n",
            "\n",
            "\n",
            "prediction =  Fe].[Fe\n",
            "ground truth = [Y]\n",
            "\n",
            "\n",
            "prediction =  Cc1ccc(N)cc1\n",
            "ground truth = CNc1ccc(C)cc1\n",
            "\n",
            "\n",
            "prediction =  CC(=O)O\n",
            "ground truth = CC(=O)O\n",
            "\n",
            "\n",
            "prediction =  CCCCC1([1-].O\n",
            "ground truth = CCCCC(=O)[O-]\n",
            "\n",
            "\n",
            "prediction =  NC(=NO)COc1ccccc1\n",
            "ground truth = NC(COc1ccccc1)=NO\n",
            "\n",
            "\n",
            "prediction =  B>>Co\n",
            "ground truth = [Co]\n",
            "\n",
            "\n",
            "prediction =  CC(=O)O\n",
            "ground truth = CC(=O)O\n",
            "\n",
            "\n",
            "prediction =  CC(=O)]\n",
            "ground truth = [H][H]\n",
            "\n",
            "\n",
            "prediction =  Cl[H[HH])([2)Cl\n",
            "ground truth = [H]C([H])(Cl)Cl\n",
            "\n",
            "\n",
            "prediction =  CCC(=O)O\n",
            "ground truth = CCC(=O)O\n",
            "\n",
            "\n",
            "prediction =  CCCCCCCCCCCCCCCCCCCCCC[Sn](Cl)(Cl)Cl\n",
            "ground truth = CCCCCCCCCCCCCCCCCCCCCC[Sn](Cl)(Cl)Cl\n",
            "\n",
            "\n",
            "prediction =  Fe3].[].3\n",
            "ground truth = [BiH3]\n",
            "\n",
            "\n",
            "prediction =  CCC(=O)O\n",
            "ground truth = CCC(=O)O\n",
            "\n",
            "\n",
            "prediction =  Cl.Cl\n",
            "ground truth = [Cl-]\n",
            "\n",
            "\n",
            "prediction =  O=O\n",
            "ground truth = O=O\n",
            "\n",
            "\n",
            "prediction =  C=C\n",
            "ground truth = C=C\n",
            "\n",
            "\n",
            "the correct rate for middle length token molecule is 0.3888888888888889\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(1,100):\n",
        "    if type(results[i][0])!=dict:\n",
        "\n",
        "        predict = [item for item in results[i]]\n",
        "\n",
        "        if len(predict)<=10:\n",
        "            predict = ''.join([item[0]['token_str'] for item in predict])\n",
        "            print('prediction = ',predict)\n",
        "            print('ground truth =',label_tst[i])\n",
        "            print('\\n')\n",
        "            total+=1\n",
        "            if predict==label_tst[i]:\n",
        "                correct+=1\n",
        "print('the correct rate for middle length token molecule is {}'.format(correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VY9CJVA3YlU",
        "outputId": "b3a49ccc-d73c-4b8c-f2cb-d66d1d9ba466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prediction =  CCC/C=C/Cn1cccc1C=O\n",
            "ground truth = CCC/C=C/Cn1cccc1C=O\n",
            "\n",
            "\n",
            "prediction =  CC=O)COP(=O)(O)O\n",
            "ground truth = CC(=O)OP(=O)(O)O\n",
            "\n",
            "\n",
            "prediction =  O=C1C[C@@H2)22CCCCCCCCCC)))))))))))))))))))))))))))))))))))))))))))))))))))c1ccccc1\n",
            "ground truth = O=C1C[C@@H](c2ccccc2)[C@]23C(=O)CCC[C@@]2(O)[C@H]([N+](=O)[O-])[C@@H](c2ccc(Br)cc2)[C@@]2(C(=O)N(Cc4ccccc4)c4ccccc42)[C@H]13\n",
            "\n",
            "\n",
            "prediction =  CCCCC#Cc1cc(F)O)O1O11111(1)(C)cc1\n",
            "ground truth = CCCCC#Cc1cc(F)ccc1C=NNS(=O)(=O)c1ccc(C)cc1\n",
            "\n",
            "\n",
            "prediction =  CC(=O)n1c2c(C)C)))))))))))))))))))))C(n2C\n",
            "ground truth = C=C(OC(C)=O)n1c(N(C)C)nc2c1c(=O)n(C)c(=O)n2C\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)C1nc2ccccc2O3OOO)CCCC)O)c1=O.O\n",
            "ground truth = CCOC(=O)[C@]1([C@H](C=O)Cc2ccccc2)Nc2ccccc2OC1=O\n",
            "\n",
            "\n",
            "prediction =  CSc1nc(SC)c2c(Br(C(((((C)C)n1\n",
            "ground truth = CSc1nc(SC)c2c(Br)nn(CCCCOC(C)=O)c2n1\n",
            "\n",
            "\n",
            "prediction =  O=C(O[C]H]HOOOOOOHOOOOO))))))))))O)c1ccccc1\n",
            "ground truth = O=C(O[C@@H](C(=O)O)[C@@H](OC(=O)c1ccccc1)C(=O)O)c1ccccc1\n",
            "\n",
            "\n",
            "prediction =  CCC1cccc2c1C1CCCCCCCC11C11111C1111C3C333)3)3C)))))))))))))))c))[C@@H]12\n",
            "ground truth = COc1cccc2c1[C@H]1O[C@@H]2[C@H]2[C@@H]1[C@@H]1[C@@H]3O[C@@H](c4cccc(OC)c43)[C@@H]12\n",
            "\n",
            "\n",
            "prediction =  CC(C)(C)OC(=NCc)cc2(ccccc2)ccn1\n",
            "ground truth = CC(C)(C)OC(=O)NCc1cc(C#N)ccn1\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)Nc1ccnCC]CCCCCCCCCCCCCCCCCCCCCCCCC))))))))))))))))))))c(=O)n1\n",
            "ground truth = CCOC(=O)Nc1ccn([C@@H]2O[C@H](CO[Si](C)(C)C(C)(C)C)[C@@H](OC(=O)CCc3ccccc3)C2(F)F)c(=O)n1\n",
            "\n",
            "\n",
            "prediction =  CCCCCCO[C@@H]1O(=OO2222OOOOOOOH](COCc2ccccc2)O1\n",
            "ground truth = CCCCCCO[C@@H]1CC(=O)[C@H](OCc2ccccc2)[C@@H](COCc2ccccc2)O1\n",
            "\n",
            "\n",
            "prediction =  COC(=O)C(C(=O)OCOC)(=(=(=(=OC)SCc1ccccc1\n",
            "ground truth = COC(=O)C(C(=O)OC)=C(SCc1ccccc1)SCc1ccccc1\n",
            "\n",
            "\n",
            "prediction =  C=CCC(O)O)OC)c((((((OCOCcc1)c)c1ccccc1\n",
            "ground truth = C=CCC(C(=O)Nc1cc(OC)c(OC)c(OC)c1)c1ccccc1\n",
            "\n",
            "\n",
            "prediction =  CCN(CC)c1ccc2c(1111))))))))))))))))))111111(=O)NCCN1CCOCC1\n",
            "ground truth = CCN(CC)c1ccc2c(c1)Oc1cc(N(CC)CC)ccc1C21c2ccccc2C(=O)N1CCOC(=O)NCCN1CCOCC1\n",
            "\n",
            "\n",
            "prediction =  Cn1c(=O)c2nHnH]33))))))))))))))))))))))))))))))))CC)C1=O\n",
            "ground truth = Cn1c(=O)c2[nH]c(-c3ccc(S(=O)(=O)N4CCN(Cc5ccc(Cl)s5)CC4)cc3)cc2n(C)c1=O\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)/C=C/c1]]]]]]]]nH)c2=O\n",
            "ground truth = CCOC(=O)/C=C/c1c(O)c(=O)[nH]c2ccccc12\n",
            "\n",
            "\n",
            "prediction =  CC(C)(CNC(=O(=CNC(=O(=))))2)n)n1\n",
            "ground truth = CC(C)(C)OC(=O)NCC(=O)NCc1nccc(N)n1\n",
            "\n",
            "\n",
            "prediction =  c1ccc(-c2ccc3c33ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc2ccccc2)cc1\n",
            "ground truth = c1ccc(N(c2ccccc2)c2c3ccccc3cc3ccc(-c4nc5ccccc5n4-c4ccccc4)cc23)cc1\n",
            "\n",
            "\n",
            "prediction =  COc1ccc(-n2cc3c#c)c4cnccc32)cc1\n",
            "ground truth = COc1ccc(-n2cc(C#N)c3cnccc32)cc1\n",
            "\n",
            "\n",
            "prediction =  CC(C)(C)C1=(=O)O)))))))))))))33(3((((3(c3)C2)C1\n",
            "ground truth = CC(C)(C)C1=NN(C(=O)NNC(=O)c2ccc(N)cc2)C(c2ccc3c(c2)OCO3)C1\n",
            "\n",
            "\n",
            "prediction =  CN1O[C@H](O)[OOOOOOOOO)[)[)[)[@H]1c1ccccc1\n",
            "ground truth = CN1O[C@@H]([N+](=O)[O-])[C@H](C#N)[C@H]1c1ccccc1\n",
            "\n",
            "\n",
            "prediction =  CCOc1cc2c(c3c1c(((((())))))))))))))))))1)))))C(C)(C)C2\n",
            "ground truth = CCOc1cc2c(c3c1OC(C)(C)C3)C(c1ccc(C(=O)OC)c(OCc3ccccc3)c1)=NC(C)(C)C2\n",
            "\n",
            "\n",
            "prediction =  C[=[N2C2CCCCC33333333)))))O)C1(C)C\n",
            "ground truth = C[N+]1=C(/C=C/c2csc3ccccc3c2=O)C(C)(C)c2ccccc21\n",
            "\n",
            "\n",
            "prediction =  O=CN(Cc1ccccc1)Cc1ccccc1\n",
            "ground truth = O=CN(Cc1ccccc1)Cc1ccccc1\n",
            "\n",
            "\n",
            "prediction =  COC(=O)Nc1cccc(-c2ccccc2c11OC(=O)OC\n",
            "ground truth = COC(=O)Nc1cc(-c2ccccc2)ccc1OC(=O)OC\n",
            "\n",
            "\n",
            "prediction =  C=CCCc1c(C(OC)OC)c)cccccccccccc))C(CC)=C\n",
            "ground truth = C=CCCc1c(C(=O)OC)c2cc(OC(C)=O)c3ccccc3c2n1CC=C\n",
            "\n",
            "\n",
            "prediction =  CCOCC1CCCC(CCCC(C)(C))SS)OOcOccccc2)C1\n",
            "ground truth = CCOCC1CCCN(CCCC(C)(C)S(=O)(=O)c2ccccc2)C1\n",
            "\n",
            "\n",
            "prediction =  c1ccc1cc2ccccc2ccccc3[nH)ccccc1ccccc1\n",
            "ground truth = C(=NNc1ccccc1)c1cc2ccccc2[nH]1\n",
            "\n",
            "\n",
            "prediction =  NC(=O)Cn+]1cccc2ccccc2ccccc21\n",
            "ground truth = NC(=O)C[n+]1cccc2ccccc21\n",
            "\n",
            "\n",
            "prediction =  CC(=O)OC(Cc1O(=O))))))))))))))))))))))))))))))))1.C(1)=O\n",
            "ground truth = CC(=O)OC(COS(C)(=O)=O)Cc1c(NC(=O)c2ccccc2)cc(OCc2ccccc2)c(NC(=O)c2ccccc2)c1C#N\n",
            "\n",
            "\n",
            "prediction =  CCC(Nc1nc2nc3c3c333333333333))))1)1)(C)cc1\n",
            "ground truth = CCC(Nc1ncnc2c1ncn2C1CCCCO1)c1nc2ccsc2c(=O)n1-c1ccc(C)cc1\n",
            "\n",
            "\n",
            "prediction =  C/C=C/C=C/C@@C](](](](]())))))))))))))))))))))))))))))))))))))))))))CCCC(C)1\n",
            "ground truth = C/C=C/C=C/C(=O)N[C@@H](COC(c1ccccc1)(c1ccc(OC)cc1)c1ccc(OC)cc1)[C@H](C)OP(OCCC#N)N(C(C)C)C(C)C\n",
            "\n",
            "\n",
            "prediction =  CC(=O2c1nc2c3c3]C]O]]]]]]]]]]]]OOO(=O(=O)n1\n",
            "ground truth = CC(=O)Nc1nc2c(c(=O)[nH]1)N=C([C@@H](O)[C@H](C)O)CN2\n",
            "\n",
            "\n",
            "prediction =  c1ccc2cc3ccccc3cc2c1\n",
            "ground truth = c1ccc2cc3ccccc3cc2c1\n",
            "\n",
            "\n",
            "prediction =  CCCCN(C)[C[C@H@HH)NCNCNCOOOO)(())(1CCCCC1\n",
            "ground truth = CCCCN(C)C(=O)[C@H](CC1CCCCC1)NC(=O)OC(C)(C)C\n",
            "\n",
            "\n",
            "prediction =  CC(C)(Br)C(=O1C1ccc)Br)cc1\n",
            "ground truth = CC(C)(Br)C(=O)c1ccc(CBr)cc1\n",
            "\n",
            "\n",
            "prediction =  O=C1CCCN2C1CCC21OCCO1\n",
            "ground truth = O=C1CCC2N1CCCC21OCCO1\n",
            "\n",
            "\n",
            "prediction =  C((=OcCOcCCC(=COcOc(=(=OcCCCOcCOcCCOCOcOc(=(OcCCOcCCOcOCOcCOCOcC\n",
            "ground truth = CC(C)(C)NC(=O)C(c1ccc(Cl)cc1)C(C(=O)Oc1ccccc1O)C(=O)Oc1ccccc1O\n",
            "\n",
            "\n",
            "prediction =  2H]cc([2([ccc)ccc)))OOO)cOcccccH)c1\n",
            "ground truth = [2H]c1ccccc1S(=O)(=O)n1c([2H])cc2nc(OC)ccc21\n",
            "\n",
            "\n",
            "prediction =  COc1c(C(C)=O)CCCCCCCCCCCCCCCCCC]1C1=O\n",
            "ground truth = COc1c(C(C)=O)ccc(F)c1[C@@H]1C[C@@H]1N=C=O\n",
            "\n",
            "\n",
            "prediction =  O[C@H]1O[@@22222)))))))))))O)OOOOOOOOOOOOOOOOO@@](H]1O1cc1\n",
            "ground truth = OC[C@H]1O[C@@H](c2ccc(Cl)c(Cc3ccc(OCCOCCN4CCOCC4)cc3)c2)[C@H](O)[C@@H](O)[C@@H]1O\n",
            "\n",
            "\n",
            "prediction =  CCCCc(=O)CSc1nnc(Cc1ccc2c33cccccccccc111(=(=(=(=(=))(C)C)cc1\n",
            "ground truth = CCCCc1nnc(SCC(=O)OC)n1Cc1ccc(-c2ccccc2C(=O)OC(C)(C)C)cc1\n",
            "\n",
            "\n",
            "prediction =  COC[C@H]1C2]2CCCCCCCCCCCCCCCCCCCC)))))))))))))))))2(1ccc(Br)cc1\n",
            "ground truth = COC[C@H]1OC(C(C)(C)C2=N[C@H](c3ccc(Br)cc3)[C@@H](COC)O2)=N[C@@H]1c1ccc(Br)cc1\n",
            "\n",
            "\n",
            "prediction =  CC(C)N1P(N)N)C(C(C(P)Cl\n",
            "ground truth = CC(C)N1P(Cl)N(C(C)C)P1Cl\n",
            "\n",
            "\n",
            "prediction =  CC(C)(C)O1O(((((1111111111111111111)Cc2o2)cc1\n",
            "ground truth = CC(C)(C)OC(=O)C(C)(C)Sc1ccc(CN(Cc2ccco2)Cc2nnc(-c3ccccc3)o2)cc1\n",
            "\n",
            "\n",
            "prediction =  C/C@C]C=CCCCCCC](](](HHHHC=C/c1=O\n",
            "ground truth = CO[C@@H](/C=C/c1ccccc1)[C@@H](C)/C=C(\\C)C=O\n",
            "\n",
            "\n",
            "prediction =  COc1cc(Cn2cc2c(OCOCOCOCOCOCOCOCOC))O)[O+](=OOOccOCc1OC\n",
            "ground truth = COc1ccc(-c2cnnn2Cc2cc(OC)c(OC)c(OC)c2)cc1[N+](=O)[O-]\n",
            "\n",
            "\n",
            "prediction =  CCC(=O)OC(=O)O)O)OOOO)))(C)C)CC1\n",
            "ground truth = CCC(=O)OC(=O)C1(NC(=O)OC(C)(C)C)CC1\n",
            "\n",
            "\n",
            "prediction =  C=C(C)ccccc1)))[](]1ccccc1cccccCl\n",
            "ground truth = C=C(C[Si](Cl)(Cl)Cl)c1ccccc1\n",
            "\n",
            "\n",
            "prediction =  Clc1c(F)c(c)c)ccccccccccccccc)c)c)c)c2c))c(F)c1Cl\n",
            "ground truth = Fc1c(F)c(F)c([Tl](Cl)c2c(F)c(F)c(F)c(F)c2F)c(F)c1F\n",
            "\n",
            "\n",
            "prediction =  CCOC(=O)CCc1c(C)ccc2ccccc12\n",
            "ground truth = CCOC(=O)CCc1c(C)ccc2ccccc12\n",
            "\n",
            "\n",
            "prediction =  CCO[C@H]1C=(COCC))](](](](](](](](](2C]2)C12CC2\n",
            "ground truth = CCO[C@H]1C=C(COC(C)=O)O[C@@H](OCc2ccccc2)C12CC2\n",
            "\n",
            "\n",
            "prediction =  CCC1=C(CC)C2CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC))C))))CCCCCCCCCCCCCCCCCC)C(CC)C)CC\n",
            "ground truth = CCC1=C(CC)c2cc3[n-]c(c(/C=C/C(O[Si](C)(C)C)C(F)(F)F)c4nc(cc5[n-]c(cc1n2)c(CC)c5CC)C(CC)C4CC)c(CC)c3CC\n",
            "\n",
            "\n",
            "the correct rate for long length molecule is 0.07407407407407407\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for i in range(1,100):\n",
        "    if type(results[i][0])!=dict:\n",
        "\n",
        "        predict = [item for item in results[i]]\n",
        "\n",
        "        if len(predict)>10:\n",
        "            predict = ''.join([item[0]['token_str'] for item in predict])\n",
        "            print('prediction = ',predict)\n",
        "            print('ground truth =',label_tst[i])\n",
        "            print('\\n')\n",
        "            total+=1\n",
        "            if predict==label_tst[i]:\n",
        "                correct+=1\n",
        "print('the correct rate for long length molecule is {}'.format(correct/total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}